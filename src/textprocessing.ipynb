{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fUeJEaZs4R7E"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "import unicodedata\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPyeoJh987yh"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    import mammoth\n",
        "except ImportError:\n",
        "    print(\"Installing mammoth for DOCX support...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"mammoth\"])\n",
        "    import mammoth\n",
        "\n",
        "try:\n",
        "    from docx import Document\n",
        "except ImportError:\n",
        "    print(\"Installing python-docx...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call([\"pip\", \"install\", \"python-docx\"])\n",
        "    from docx import Document\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    \"\"\"Data class to represent a text chunk with metadata\"\"\"\n",
        "    id: str\n",
        "    text: str\n",
        "    cleaned_text: str\n",
        "    metadata: Dict[str, Any]\n",
        "    word_count: int\n",
        "    char_count: int\n",
        "\n",
        "\n",
        "class ArabicTextPreprocessor:\n",
        "    \"\"\"\n",
        "    Comprehensive Arabic text preprocessing class for RAG applications.\n",
        "    Handles text extraction, cleaning, chunking, and metadata generation.\n",
        "\n",
        "    **NEW:**\n",
        "    ‚Ä¢ Added removal of phone numbers, e‚Äëmail addresses, and ‚Äúflixat‚Äù (URLs/links).\n",
        "    ‚Ä¢ Added three helper regex patterns and corresponding removal functions.\n",
        "    ‚Ä¢ `clean_text()` now supports the optional flags:\n",
        "        - `remove_phone_numbers`\n",
        "        - `remove_emails`\n",
        "        - `remove_flixat`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Initialize the preprocessor\n",
        "\n",
        "        Args:\n",
        "            chunk_size: Maximum number of words per chunk\n",
        "            chunk_overlap: Number of words to overlap between chunks\n",
        "        \"\"\"\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Arabic diacritics (tashkeel) pattern\n",
        "        self.diacritics_pattern = re.compile(r'[\\u064B-\\u0652\\u0670\\u0640]')\n",
        "\n",
        "        # Tatweel (kashida) pattern\n",
        "        self.tatweel_pattern = re.compile(r'\\u0640+')\n",
        "\n",
        "        # Multiple spaces/newlines pattern\n",
        "        self.whitespace_pattern = re.compile(r'\\s+')\n",
        "\n",
        "        # Non‚ÄëArabic characters (keeping numbers and basic punctuation)\n",
        "        self.non_arabic_pattern = re.compile(\n",
        "            r'[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFDFF\\uFE70-\\uFEFF\\s\\d\\.\\ÿå\\ÿü\\!\\:\\;\\(\\)\\[\\]\\{\\}\\\"\\']+'\n",
        "        )\n",
        "\n",
        "        # Punctuation normalization\n",
        "        self.punctuation_map = {\n",
        "            'ÿü': '?',\n",
        "            'ÿå': ',',\n",
        "            'ÿõ': ';',\n",
        "            'Ÿ™': '%',\n",
        "            'Ÿ´': ',',\n",
        "            'Ÿ¨': ',',\n",
        "            '€î': '.',\n",
        "        }\n",
        "\n",
        "        # === NEW REGEX PATTERNS ===\n",
        "        # Phone numbers (handles international prefix, spaces, dashes, dots)\n",
        "        self.phone_pattern = re.compile(\n",
        "            r'\\b(?:\\+?[\\d]{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?){1,4}\\d{3,4}\\b'\n",
        "        )\n",
        "\n",
        "        # E‚Äëmail addresses\n",
        "        self.email_pattern = re.compile(\n",
        "            r'\\b[\\w\\.-]+?@[\\w\\.-]+\\.\\w{2,}\\b'\n",
        "        )\n",
        "\n",
        "        # ‚ÄúFlixat‚Äù ‚Üí interpreted here as URLs / web links\n",
        "        self.flixat_pattern = re.compile(\n",
        "            r'(?:https?://\\S+|www\\.\\S+)'\n",
        "        )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                        EXTRACTION METHODS\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def extract_text_from_docx(self, file_path: str, use_uploaded_file: bool = True) -> tuple[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extract text from DOCX file using multiple methods for better coverage\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the DOCX file (or filename if uploaded)\n",
        "            use_uploaded_file: Whether to read from uploaded files via window.fs\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (extracted_text, metadata)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if use_uploaded_file:\n",
        "                # Read uploaded file using window.fs.readFile\n",
        "                try:\n",
        "                    import js\n",
        "                    file_data = js.window.fs.readFile(file_path)\n",
        "\n",
        "                    # Convert to bytes for mammoth\n",
        "                    import io\n",
        "                    file_bytes = io.BytesIO(file_data.to_py())\n",
        "\n",
        "                    # Method 1: Using mammoth (better formatting preservation)\n",
        "                    result = mammoth.extract_raw_text(file_bytes)\n",
        "                    text_mammoth = result.value\n",
        "\n",
        "                    # Method 2: Using python-docx (backup method)\n",
        "                    file_bytes.seek(0)  # Reset to beginning\n",
        "                    doc = Document(file_bytes)\n",
        "                    text_docx = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "                    extracted_text = text_mammoth if len(text_mammoth) > len(text_docx) else text_docx\n",
        "                    file_size = len(file_data.to_py())\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with window.fs method, trying alternative approach: {e}\")\n",
        "                    # Fallback: try to read as if it's a local file\n",
        "                    return self._extract_from_local_file(file_path)\n",
        "            else:\n",
        "                return self._extract_from_local_file(file_path)\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = {\n",
        "                'source_file': file_path,\n",
        "                'file_path': file_path,\n",
        "                'extraction_method': 'mammoth' if len(text_mammoth) > len(text_docx) else 'python-docx',\n",
        "                'extraction_timestamp': datetime.now().isoformat(),\n",
        "                'original_length': len(extracted_text),\n",
        "                'file_size_bytes': file_size,\n",
        "            }\n",
        "\n",
        "            # Try to extract document properties if available\n",
        "            try:\n",
        "                core_props = doc.core_properties\n",
        "                if core_props.title:\n",
        "                    metadata['title'] = core_props.title\n",
        "                if core_props.author:\n",
        "                    metadata['author'] = core_props.author\n",
        "                if core_props.created:\n",
        "                    metadata['created'] = core_props.created.isoformat()\n",
        "                if core_props.modified:\n",
        "                    metadata['modified'] = core_props.modified.isoformat()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            return extracted_text, metadata\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Error extracting text from {file_path}: {str(e)}\")\n",
        "\n",
        "    def _extract_from_local_file(self, file_path: str) -> tuple[str, Dict[str, Any]]:\n",
        "        \"\"\"Extract text from local file system\"\"\"\n",
        "        # Method 1: Using mammoth (better formatting preservation)\n",
        "        with open(file_path, \"rb\") as docx_file:\n",
        "            result = mammoth.extract_raw_text(docx_file)\n",
        "            text_mammoth = result.value\n",
        "\n",
        "        # Method 2: Using python-docx (backup method)\n",
        "        doc = Document(file_path)\n",
        "        text_docx = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "        # Use the longer text (usually mammoth gives better results)\n",
        "        extracted_text = text_mammoth if len(text_mammoth) > len(text_docx) else text_docx\n",
        "\n",
        "        # Extract metadata\n",
        "        metadata = {\n",
        "            'source_file': Path(file_path).name,\n",
        "            'file_path': file_path,\n",
        "            'extraction_method': 'mammoth' if len(text_mammoth) > len(text_docx) else 'python-docx',\n",
        "            'extraction_timestamp': datetime.now().isoformat(),\n",
        "            'original_length': len(extracted_text),\n",
        "            'file_size_bytes': Path(file_path).stat().st_size,\n",
        "        }\n",
        "\n",
        "        # Try to extract document properties if available\n",
        "        try:\n",
        "            core_props = doc.core_properties\n",
        "            if core_props.title:\n",
        "                metadata['title'] = core_props.title\n",
        "            if core_props.author:\n",
        "                metadata['author'] = core_props.author\n",
        "            if core_props.created:\n",
        "                metadata['created'] = core_props.created.isoformat()\n",
        "            if core_props.modified:\n",
        "                metadata['modified'] = core_props.modified.isoformat()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return extracted_text, metadata\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                        CLEANING HELPERS (NEW)\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def remove_phone_numbers(self, text: str) -> str:\n",
        "        \"\"\"Remove phone numbers from text\"\"\"\n",
        "        return self.phone_pattern.sub(' ', text)\n",
        "\n",
        "    def remove_emails(self, text: str) -> str:\n",
        "        \"\"\"Remove e‚Äëmail addresses from text\"\"\"\n",
        "        return self.email_pattern.sub(' ', text)\n",
        "\n",
        "    def remove_flixat(self, text: str) -> str:\n",
        "        \"\"\"Remove URLs / links (‚Äúflixat‚Äù) from text\"\"\"\n",
        "        return self.flixat_pattern.sub(' ', text)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                        EXISTING CLEANERS\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def remove_diacritics(self, text: str) -> str:\n",
        "        \"\"\"Remove Arabic diacritics (tashkeel) from text\"\"\"\n",
        "        return self.diacritics_pattern.sub('', text)\n",
        "\n",
        "    def remove_tatweel(self, text: str) -> str:\n",
        "        \"\"\"Remove tatweel (kashida) characters\"\"\"\n",
        "        return self.tatweel_pattern.sub('', text)\n",
        "\n",
        "    def normalize_punctuation(self, text: str) -> str:\n",
        "        \"\"\"Normalize Arabic punctuation to standard forms\"\"\"\n",
        "        for arabic_punct, standard_punct in self.punctuation_map.items():\n",
        "            text = text.replace(arabic_punct, standard_punct)\n",
        "        return text\n",
        "\n",
        "    def remove_non_arabic(self, text: str) -> str:\n",
        "        \"\"\"Remove non-Arabic characters while preserving numbers and basic punctuation\"\"\"\n",
        "        return self.non_arabic_pattern.sub(' ', text)\n",
        "\n",
        "    def normalize_whitespace(self, text: str) -> str:\n",
        "        \"\"\"Normalize whitespace (multiple spaces, tabs, newlines)\"\"\"\n",
        "        return self.whitespace_pattern.sub(' ', text).strip()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                            CLEAN TEXT\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def clean_text(\n",
        "        self,\n",
        "        text: str,\n",
        "        remove_diacritics: bool = True,\n",
        "        remove_tatweel: bool = True,\n",
        "        normalize_punctuation: bool = True,\n",
        "        remove_non_arabic: bool = False,\n",
        "        normalize_whitespace: bool = True,\n",
        "        # NEW flags\n",
        "        remove_phone_numbers: bool = False,\n",
        "        remove_emails: bool = False,\n",
        "        remove_flixat: bool = False,\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Apply comprehensive text cleaning\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            remove_diacritics: Remove Arabic diacritics\n",
        "            remove_tatweel: Remove tatweel characters\n",
        "            normalize_punctuation: Normalize punctuation\n",
        "            remove_non_arabic: Remove non-Arabic characters\n",
        "            normalize_whitespace: Normalize whitespace\n",
        "            remove_phone_numbers: Remove phone numbers\n",
        "            remove_emails: Remove e‚Äëmail addresses\n",
        "            remove_flixat: Remove URLs / links\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        cleaned = text\n",
        "\n",
        "        if remove_diacritics:\n",
        "            cleaned = self.remove_diacritics(cleaned)\n",
        "\n",
        "        if remove_tatweel:\n",
        "            cleaned = self.remove_tatweel(cleaned)\n",
        "\n",
        "        if normalize_punctuation:\n",
        "            cleaned = self.normalize_punctuation(cleaned)\n",
        "\n",
        "        if remove_non_arabic:\n",
        "            cleaned = self.remove_non_arabic(cleaned)\n",
        "\n",
        "        # --- NEW removals ---\n",
        "        if remove_phone_numbers:\n",
        "            cleaned = self.remove_phone_numbers(cleaned)\n",
        "\n",
        "        if remove_emails:\n",
        "            cleaned = self.remove_emails(cleaned)\n",
        "\n",
        "        if remove_flixat:\n",
        "            cleaned = self.remove_flixat(cleaned)\n",
        "\n",
        "        if normalize_whitespace:\n",
        "            cleaned = self.normalize_whitespace(cleaned)\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #        SENTENCE / PARAGRAPH SPLITTING + CHUNKING (unchanged)\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def split_into_sentences(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentences using Arabic sentence delimiters\"\"\"\n",
        "        sentence_endings = r'[.!?ÿü€î]+'\n",
        "        sentences = re.split(sentence_endings, text)\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        return sentences\n",
        "\n",
        "    def chunk_text(self, text: str, method: str = 'word_based') -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks for RAG processing\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            method: Chunking method ('word_based', 'sentence_based', or 'paragraph_based')\n",
        "\n",
        "        Returns:\n",
        "            List of text chunks\n",
        "        \"\"\"\n",
        "        if method == 'word_based':\n",
        "            return self._word_based_chunking(text)\n",
        "        elif method == 'sentence_based':\n",
        "            return self._sentence_based_chunking(text)\n",
        "        elif method == 'paragraph_based':\n",
        "            return self._paragraph_based_chunking(text)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown chunking method: {method}\")\n",
        "\n",
        "    def _word_based_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into word-based chunks with overlap\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
        "            chunk_words = words[i:i + self.chunk_size]\n",
        "            chunk_text = ' '.join(chunk_words)\n",
        "            if chunk_text.strip():\n",
        "                chunks.append(chunk_text)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _sentence_based_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into sentence-based chunks\"\"\"\n",
        "        sentences = self.split_into_sentences(text)\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_word_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_words = len(sentence.split())\n",
        "\n",
        "            if current_word_count + sentence_words > self.chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = current_chunk[-1:] if current_chunk else []\n",
        "                current_word_count = len(' '.join(current_chunk).split())\n",
        "\n",
        "            current_chunk.append(sentence)\n",
        "            current_word_count += sentence_words\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _paragraph_based_chunking(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into paragraph-based chunks\"\"\"\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_word_count = 0\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph_words = len(paragraph.split())\n",
        "\n",
        "            if current_word_count + paragraph_words > self.chunk_size and current_chunk:\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = []\n",
        "                current_word_count = 0\n",
        "\n",
        "            current_chunk.append(paragraph)\n",
        "            current_word_count += paragraph_words\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append('\\n\\n'.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                          UTILITIES\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def generate_chunk_id(self, text: str, index: int) -> str:\n",
        "        \"\"\"Generate unique ID for chunk\"\"\"\n",
        "        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]\n",
        "        return f\"chunk_{index:04d}_{text_hash}\"\n",
        "\n",
        "    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract potential keywords from Arabic text\n",
        "        (Simple frequency-based approach - can be enhanced with NLP libraries)\n",
        "        \"\"\"\n",
        "        cleaned = self.clean_text(text)\n",
        "        words = cleaned.split()\n",
        "\n",
        "        arabic_stopwords = {\n",
        "            'ŸÅŸä', 'ŸÖŸÜ', 'ÿ•ŸÑŸâ', 'ÿπŸÑŸâ', 'ÿπŸÜ', 'ŸÖÿπ', 'Ÿáÿ∞ÿß', 'Ÿáÿ∞Ÿá', 'ÿ∞ŸÑŸÉ', 'ÿ™ŸÑŸÉ',\n",
        "            'ÿßŸÑÿ™Ÿä', 'ÿßŸÑÿ∞Ÿä', 'ÿßŸÑÿ™Ÿä', 'ÿßŸÑÿ∞ŸäŸÜ', 'ÿßŸÑŸÑÿ™ÿßŸÜ', 'ÿßŸÑŸÑÿ∞ÿßŸÜ', 'ÿßŸÑŸÑŸàÿßÿ™Ÿä', 'ÿßŸÑŸÑÿßÿ™Ÿä',\n",
        "            'ŸÉÿßŸÜ', 'ŸÉÿßŸÜÿ™', 'ŸäŸÉŸàŸÜ', 'ÿ™ŸÉŸàŸÜ', 'ÿ£ŸÜ', 'ÿ•ŸÜ', 'ŸÑŸÉŸÜ', 'ŸàŸÑŸÉŸÜ', 'ÿ£ŸÖ', 'ÿ£ŸÖÿß',\n",
        "            'ŸÑÿß', 'ŸÖÿß', 'ŸÑŸÖ', 'ŸÑŸÜ', 'ŸÇÿØ', 'ŸÅŸÇÿØ', 'ŸàŸÇÿØ', 'ŸÜÿØ', 'ÿπŸÜÿØ', 'ÿ®ÿπÿØ', 'ŸÇÿ®ŸÑ'\n",
        "        }\n",
        "\n",
        "        word_freq: Dict[str, int] = {}\n",
        "        for word in words:\n",
        "            if len(word) > 2 and word not in arabic_stopwords:\n",
        "                word_freq[word] = word_freq.get(word, 0) + 1\n",
        "\n",
        "        keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [word for word, freq in keywords[:top_k]]\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                       MAIN PROCESSING PIPELINE\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def process_document(\n",
        "        self,\n",
        "        file_path: str,\n",
        "        chunking_method: str = 'sentence_based',\n",
        "        cleaning_options: Optional[Dict[str, bool]] = None,\n",
        "        use_uploaded_file: bool = True\n",
        "    ) -> List[TextChunk]:\n",
        "        \"\"\"\n",
        "        Complete document processing pipeline\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the document (or filename if uploaded)\n",
        "            chunking_method: Method for text chunking\n",
        "            cleaning_options: Dictionary of cleaning options\n",
        "            use_uploaded_file: Whether to read from uploaded files\n",
        "\n",
        "        Returns:\n",
        "            List of TextChunk objects\n",
        "        \"\"\"\n",
        "        # Default cleaning options\n",
        "        if cleaning_options is None:\n",
        "            cleaning_options = {\n",
        "                'remove_diacritics': True,\n",
        "                'remove_tatweel': True,\n",
        "                'normalize_punctuation': True,\n",
        "                'remove_non_arabic': False,\n",
        "                'normalize_whitespace': True,\n",
        "                # NEW flags (disabled by default to preserve behaviour)\n",
        "                'remove_phone_numbers': False,\n",
        "                'remove_emails': False,\n",
        "                'remove_flixat': False\n",
        "            }\n",
        "\n",
        "        print(f\"Processing document: {file_path}\")\n",
        "\n",
        "        # Step 1: Extract text\n",
        "        print(\"1. Extracting text...\")\n",
        "        original_text, base_metadata = self.extract_text_from_docx(file_path, use_uploaded_file)\n",
        "        print(f\"   Extracted {len(original_text)} characters\")\n",
        "\n",
        "        # Step 2: Clean text\n",
        "        print(\"2. Cleaning text...\")\n",
        "        cleaned_text = self.clean_text(original_text, **cleaning_options)\n",
        "        print(f\"   Cleaned text: {len(cleaned_text)} characters\")\n",
        "\n",
        "        # Step 3: Chunk text\n",
        "        print(f\"3. Chunking text using {chunking_method} method...\")\n",
        "        chunks = self.chunk_text(cleaned_text, method=chunking_method)\n",
        "        print(f\"   Created {len(chunks)} chunks\")\n",
        "\n",
        "        # Step 4: Create TextChunk objects with metadata\n",
        "        print(\"4. Generating metadata...\")\n",
        "        text_chunks: List[TextChunk] = []\n",
        "\n",
        "        for i, chunk_text in enumerate(chunks):\n",
        "            chunk_id = self.generate_chunk_id(chunk_text, i)\n",
        "\n",
        "            cleaned_chunk = self.clean_text(chunk_text, **cleaning_options)\n",
        "\n",
        "            word_count = len(chunk_text.split())\n",
        "            char_count = len(chunk_text)\n",
        "\n",
        "            keywords = self.extract_keywords(cleaned_chunk, top_k=5)\n",
        "\n",
        "            metadata = {\n",
        "                **base_metadata,\n",
        "                'chunk_index': i,\n",
        "                'total_chunks': len(chunks),\n",
        "                'chunking_method': chunking_method,\n",
        "                'cleaning_options': cleaning_options,\n",
        "                'keywords': keywords,\n",
        "                'position_start': sum(len(c.split()) for c in chunks[:i]),\n",
        "                'position_end': sum(len(c.split()) for c in chunks[:i + 1]),\n",
        "            }\n",
        "\n",
        "            text_chunks.append(\n",
        "                TextChunk(\n",
        "                    id=chunk_id,\n",
        "                    text=chunk_text,\n",
        "                    cleaned_text=cleaned_chunk,\n",
        "                    metadata=metadata,\n",
        "                    word_count=word_count,\n",
        "                    char_count=char_count\n",
        "                )\n",
        "            )\n",
        "\n",
        "        print(f\"‚úì Processing complete: {len(text_chunks)} chunks created\")\n",
        "        return text_chunks\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    #                     SAVE / LOAD CHUNKS (unchanged)\n",
        "    # ------------------------------------------------------------------\n",
        "\n",
        "    def save_processed_chunks(self, chunks: List[TextChunk], output_path: str):\n",
        "        \"\"\"Save processed chunks to JSON file\"\"\"\n",
        "        chunks_data = []\n",
        "        for chunk in chunks:\n",
        "            chunks_data.append({\n",
        "                'id': chunk.id,\n",
        "                'text': chunk.text,\n",
        "                'cleaned_text': chunk.cleaned_text,\n",
        "                'metadata': chunk.metadata,\n",
        "                'word_count': chunk.word_count,\n",
        "                'char_count': chunk.char_count\n",
        "            })\n",
        "\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Chunks saved to: {output_path}\")\n",
        "\n",
        "    def load_processed_chunks(self, input_path: str) -> List[TextChunk]:\n",
        "        \"\"\"Load processed chunks from JSON file\"\"\"\n",
        "        with open(input_path, 'r', encoding='utf-8') as f:\n",
        "            chunks_data = json.load(f)\n",
        "\n",
        "        chunks = []\n",
        "        for data in chunks_data:\n",
        "            chunks.append(\n",
        "                TextChunk(\n",
        "                    id=data['id'],\n",
        "                    text=data['text'],\n",
        "                    cleaned_text=data['cleaned_text'],\n",
        "                    metadata=data['metadata'],\n",
        "                    word_count=data['word_count'],\n",
        "                    char_count=data['char_count']\n",
        "                )\n",
        "            )\n",
        "        return chunks\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "########################################################################################################################################## EXAMPLE USAGE (unchanged ‚Äî optional demonstration)\n",
        "# ----------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    preprocessor = ArabicTextPreprocessor(chunk_size=400, chunk_overlap=50)\n",
        "\n",
        "    file_path = \"/content/ŸÅÿ™ÿ≠-ÿßŸÑŸÖÿ∫Ÿäÿ´-ÿ®ÿ¥ÿ±ÿ≠-ÿ£ŸÑŸÅŸäÿ©-ÿßŸÑÿ≠ÿØŸäÿ´2.docx\"\n",
        "\n",
        "    try:\n",
        "        chunks = preprocessor.process_document(\n",
        "            file_path=file_path,\n",
        "            chunking_method='sentence_based',\n",
        "            cleaning_options={\n",
        "                'remove_diacritics': True,\n",
        "                'remove_tatweel': True,\n",
        "                'normalize_punctuation': True,\n",
        "                'remove_non_arabic': False,\n",
        "                'normalize_whitespace': True,\n",
        "                # Enable new flags here if desired:\n",
        "                'remove_phone_numbers': True,\n",
        "                'remove_emails': True,\n",
        "                'remove_flixat': True\n",
        "            },\n",
        "            use_uploaded_file=True\n",
        "        )\n",
        "\n",
        "        print(f\"\\nüìä Processing Results:\")\n",
        "        print(f\"   ‚Ä¢ Total chunks: {len(chunks)}\")\n",
        "        print(f\"   ‚Ä¢ Average words per chunk: {sum(c.word_count for c in chunks) / len(chunks):.1f}\")\n",
        "        print(f\"   ‚Ä¢ Total words: {sum(c.word_count for c in chunks)}\")\n",
        "\n",
        "        if chunks:\n",
        "            sample = chunks[0]\n",
        "            print(f\"\\nüìù Sample Chunk (First):\")\n",
        "            print(f\"   ‚Ä¢ ID: {sample.id}\")\n",
        "            print(f\"   ‚Ä¢ Words: {sample.word_count}\")\n",
        "            print(f\"   ‚Ä¢ Keywords: {', '.join(sample.metadata.get('keywords', []))}\")\n",
        "            print(f\"   ‚Ä¢ Text preview: {sample.cleaned_text[:200]}...\")\n",
        "##############################################################################################################################\n",
        "        output_file = \"processed_chunks 32.json\"\n",
        "        preprocessor.save_processed_chunks(chunks, output_file)\n",
        "\n",
        "        print(f\"\\n‚úÖ Processing complete! Chunks saved to '{output_file}'\")\n",
        "        print(\"üí° You can now use these chunks in your RAG model\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: File '{file_path}' not found\")\n",
        "        print(\"üìå Make sure the file is in the same directory as this script\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing document: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
